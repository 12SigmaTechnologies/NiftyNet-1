{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:TensorFlow version 1.6.0\n",
      "CRITICAL:tensorflow:Optional Python module cv2 not found, please install cv2 and retry if the application fails.\n",
      "INFO:tensorflow:Available Image Loaders:\n",
      "['nibabel', 'skimage', 'pillow', 'simpleitk', 'dummy'].\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Optional Python module yaml not found, please install yaml and retry if the application fails.\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Optional Python module yaml version None not found, please install yaml-None and retry if the application fails.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "niftynet_path = '/home/tom/phd/NiftyNet-Generator-PR/NiftyNet'\n",
    "sys.path.append(niftynet_path)\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from niftynet.io.image_reader import ImageReader\n",
    "from niftynet.engine.image_window_dataset import ImageWindowDatasetCSV\n",
    "from niftynet.engine.sampler_resize_v2 import ResizeSampler\n",
    "from niftynet.io.csv_reader import CSVReader\n",
    "from niftynet.contrib.dataset_sampler.preprocessing import Preprocessing\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing: https://github.com/NifTK/NiftyNetModelZoo\n",
      "mr_ct_regression_model_zoo_data: OK. \n",
      "Already downloaded. Use the -r option to download again.\n"
     ]
    }
   ],
   "source": [
    "from niftynet.utilities.download import download\n",
    "download('mr_ct_regression_model_zoo_data')\n",
    "labels_location = 'ct.csv'\n",
    "files = [file for file in os.listdir('/home/tom/niftynet/data/mr_ct_regression/CT_zero_mean') if file.endswith('.nii.gz')]\n",
    "pd.DataFrame(data=[(file, file.replace('.nii.gz', '')) for file in files]).to_csv('ct.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image']\n",
      "\u001b[1mINFO:niftynet:\u001b[0m \n",
      "\n",
      "Number of subjects 15, input section names: ['subject_id', 'CT']\n",
      "-- using all subjects (without data partitioning).\n",
      "\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Image reader: loading 15 subjects from sections ['CT'] as input [image]\n"
     ]
    }
   ],
   "source": [
    "NetParam = namedtuple('NetParam', 'normalise_foreground_only foreground_type multimod_foreground_type histogram_ref_file norm_type cutoff normalisation whitening')\n",
    "ActionParam = namedtuple('ActionParam', 'random_flipping_axes scaling_percentage rotation_angle rotation_angle_x rotation_angle_y rotation_angle_z do_elastic_deformation num_ctrl_points deformation_sigma proportion_to_deform')\n",
    "\n",
    "        \n",
    "class TaskParam:\n",
    "    def __init__(self, classes):\n",
    "        self.image = classes\n",
    "net_param = NetParam(normalise_foreground_only=False,\n",
    "                     foreground_type='threshold_plus',\n",
    "                     multimod_foreground_type = 'and',\n",
    "                     histogram_ref_file='mapping.txt',\n",
    "                     norm_type='percentile',\n",
    "                     cutoff=(0.05, 0.95),\n",
    "                     normalisation=False,\n",
    "                     whitening=True\n",
    "                    )\n",
    "action_param = ActionParam(random_flipping_axes=[],\n",
    "                           scaling_percentage=[],\n",
    "                           rotation_angle=None,\n",
    "                           rotation_angle_x=None,\n",
    "                           rotation_angle_y=None,\n",
    "                           rotation_angle_z=None,\n",
    "                           do_elastic_deformation=False,\n",
    "                           num_ctrl_points=6,\n",
    "                           deformation_sigma=50,\n",
    "                           proportion_to_deform=0.9)\n",
    "\n",
    "task_param = {'image': {'image':True}}\n",
    "task_param = TaskParam(['image'])\n",
    "print(vars(task_param).get('image'))\n",
    "# creating an image reader.\n",
    "data_param = {'CT': {'path_to_search': '~/niftynet/data/mr_ct_regression/CT_zero_mean',\n",
    "            'filename_contains': 'nii'}}\n",
    "grouping_param = {'image': (['CT'])}\n",
    "\n",
    "image_reader = ImageReader().initialise(data_param, grouping_param)\n",
    "preprocessing = Preprocessing(net_param, action_param, task_param)\n",
    "normalisation_layers = preprocessing.prepare_normalisation_layers()\n",
    "augmentation_layers = preprocessing.prepare_augmentation_layers()\n",
    "image_reader.add_preprocessing_layers(normalisation_layers + augmentation_layers)\n",
    "csv_reader = CSVReader().initialise(labels_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 8, 16]\n",
      "\u001b[1mINFO:niftynet:\u001b[0m reading size of preprocessed images\n",
      "\u001b[1mWARNING:niftynet:\u001b[0m queue_length should be larger than batch_size, defaulting to batch_size * 5.0 (500).\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Initiating dataset...\n",
      "\u001b[1mINFO:niftynet:\u001b[0m self.from_generator: True\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Initiating dataset from generator...\n",
      "Num Parallel Calls: 2\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 1 / 10\n",
      "Time per batch: 25.9870502948761\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 2 / 10\n",
      "Time per batch: 4.30904746055603\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 3 / 10\n",
      "Time per batch: 4.451494932174683\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 4 / 10\n",
      "Time per batch: 4.402526378631592\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 5 / 10\n",
      "Time per batch: 4.456684112548828\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 6 / 10\n",
      "Time per batch: 4.406505346298218\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 7 / 10\n",
      "Time per batch: 4.4387054443359375\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 8 / 10\n",
      "Time per batch: 4.466816663742065\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 9 / 10\n",
      "Time per batch: 4.434375047683716\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 10 / 10\n",
      "Time per batch: 4.4926910400390625\n",
      "Mean batch time: 4.428760714001125\n",
      "\u001b[1mINFO:niftynet:\u001b[0m reading size of preprocessed images\n",
      "\u001b[1mWARNING:niftynet:\u001b[0m queue_length should be larger than batch_size, defaulting to batch_size * 5.0 (500).\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Initiating dataset...\n",
      "\u001b[1mINFO:niftynet:\u001b[0m self.from_generator: True\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Initiating dataset from generator...\n",
      "Num Parallel Calls: 4\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 1 / 10\n",
      "Time per batch: 14.725233316421509\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 2 / 10\n",
      "Time per batch: 2.5354156494140625\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 3 / 10\n",
      "Time per batch: 2.524552822113037\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 4 / 10\n",
      "Time per batch: 2.3708324432373047\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 5 / 10\n",
      "Time per batch: 2.4221031665802\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 6 / 10\n",
      "Time per batch: 2.48394513130188\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 7 / 10\n",
      "Time per batch: 2.456294059753418\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 8 / 10\n",
      "Time per batch: 2.394380807876587\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 9 / 10\n",
      "Time per batch: 2.414968490600586\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 10 / 10\n",
      "Time per batch: 2.4265353679656982\n",
      "Mean batch time: 2.4476697709825306\n",
      "\u001b[1mINFO:niftynet:\u001b[0m reading size of preprocessed images\n",
      "\u001b[1mWARNING:niftynet:\u001b[0m queue_length should be larger than batch_size, defaulting to batch_size * 5.0 (500).\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Initiating dataset...\n",
      "\u001b[1mINFO:niftynet:\u001b[0m self.from_generator: True\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Initiating dataset from generator...\n",
      "Num Parallel Calls: 8\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 1 / 10\n",
      "Time per batch: 7.562370777130127\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 2 / 10\n",
      "Time per batch: 1.204786777496338\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 3 / 10\n",
      "Time per batch: 1.3247928619384766\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 4 / 10\n",
      "Time per batch: 1.256899118423462\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 5 / 10\n",
      "Time per batch: 1.2707467079162598\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 6 / 10\n",
      "Time per batch: 1.304060935974121\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 7 / 10\n",
      "Time per batch: 1.2477483749389648\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 8 / 10\n",
      "Time per batch: 1.3253648281097412\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 9 / 10\n",
      "Time per batch: 1.3146610260009766\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 10 / 10\n",
      "Time per batch: 1.2469687461853027\n",
      "Mean batch time: 1.277336597442627\n",
      "\u001b[1mINFO:niftynet:\u001b[0m reading size of preprocessed images\n",
      "\u001b[1mWARNING:niftynet:\u001b[0m queue_length should be larger than batch_size, defaulting to batch_size * 5.0 (500).\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Initiating dataset...\n",
      "\u001b[1mINFO:niftynet:\u001b[0m self.from_generator: True\n",
      "\u001b[1mINFO:niftynet:\u001b[0m Initiating dataset from generator...\n",
      "Num Parallel Calls: 16\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 1 / 10\n",
      "Time per batch: 6.557478189468384\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 2 / 10\n",
      "Time per batch: 1.01448655128479\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 3 / 10\n",
      "Time per batch: 1.0860965251922607\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 4 / 10\n",
      "Time per batch: 0.9710178375244141\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 5 / 10\n",
      "Time per batch: 1.0350019931793213\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 6 / 10\n",
      "Time per batch: 1.1063556671142578\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 7 / 10\n",
      "Time per batch: 1.1326797008514404\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 8 / 10\n",
      "Time per batch: 1.0397119522094727\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 9 / 10\n",
      "Time per batch: 1.068472146987915\n",
      "(100, 1, 100, 100, 100, 1, 1) (100, 1, 16, 1, 1, 1, 1)\n",
      "Batch 10 / 10\n",
      "Time per batch: 1.0207734107971191\n",
      "Mean batch time: 1.0527328650156658\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_parallel_calls = [2, 4, 8, 16]\n",
    "print(num_parallel_calls)\n",
    "total_times_dict = {}\n",
    "batches = 10\n",
    "batch_size = 100\n",
    "for num_parallel_call in num_parallel_calls:\n",
    "    window_sizes = {'image': (100, 100, 100), 'label': (1, 1, 1)}\n",
    "    sampler = ResizeSampler(reader=image_reader,\n",
    "                            csv_reader=csv_reader,\n",
    "                            window_sizes=window_sizes,\n",
    "                            num_threads=num_parallel_call,\n",
    "                            smaller_final_batch_mode='drop',\n",
    "                            batch_size=batch_size,\n",
    "                            queue_length=num_parallel_call)\n",
    "    next_window = sampler.pop_batch_op()\n",
    "    with tf.Session() as sess:\n",
    "        print('Num Parallel Calls: {}'.format(num_parallel_call))\n",
    "        t0 = time.time()\n",
    "        batch_times = []\n",
    "        sess.run(sampler.iterator.make_initializer(sampler.dataset))\n",
    "        for i in range(batches):\n",
    "            try:\n",
    "                value = sess.run(next_window)\n",
    "                print(value['image'].shape, value['label'].shape)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            batch_time = time.time() - t0\n",
    "            batch_times.append(batch_time)\n",
    "            print('Batch {} / {}'.format(i+1, batches))\n",
    "            print('Time per batch: {}'.format(batch_time))\n",
    "            t0 = time.time()\n",
    "        total_times_dict[num_parallel_call] = batch_times\n",
    "        print('Mean batch time: {}'.format(sum(batch_times[1:])/len(batch_times[1:])))\n",
    "    if sampler.enqueuer is not None:\n",
    "        sampler.enqueuer.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "to_plot = [2, 4, 8, 16]\n",
    "means = [np.mean(total_times_dict[num][1:]) for num in to_plot]\n",
    "ideal = [np.mean(total_times_dict[num][1:]) * 2 / num for num in to_plot]\n",
    "plt.plot(to_plot, means, label='observed')\n",
    "plt.plot(to_plot, ideal, label='ideal')\n",
    "plt.title('Mean time per image as threads increases for 80 thread machine')\n",
    "plt.xlabel('Threads')\n",
    "plt.ylabel('mean time')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
